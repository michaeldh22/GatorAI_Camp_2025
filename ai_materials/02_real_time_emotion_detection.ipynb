{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b311103",
   "metadata": {},
   "source": [
    "# ðŸŽ® Real-Time Emotion Detection for Game Integration\n",
    "\n",
    "This notebook takes our trained emotion recognition model and applies it to a live webcam feed. The goal is to detect faces, predict their emotions in real-time, and prepare the output for integration with our adventure game.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Load our trained model** (`emotion_model.pth`)\n",
    "2.  **Access the webcam** for live video\n",
    "3.  **Detect faces** in each frame using OpenCV\n",
    "4.  **Preprocess** each face for our model\n",
    "5.  **Predict the emotion** and display it on the screen\n",
    "6.  **Discuss game integration** strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff251fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import requests  # For downloading the face detector\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af92f4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EmotionCNN model class defined.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEFINE THE CNN MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "# We need to define the model architecture exactly as it was during training\n",
    "# so we can load the saved weights correctly.\n",
    "\n",
    "\n",
    "class EmotionCNN(pl.LightningModule):\n",
    "    \"\"\"Compact CNN for emotion recognition using PyTorch Lightning\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=6, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Convolutional layers with batch normalization\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 48x48 -> 24x24\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 24x24 -> 12x12\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 12x12 -> 6x6\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling (more efficient than flattening)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # 6x6x128 -> 1x1x128\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"âœ… EmotionCNN model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb709c",
   "metadata": {},
   "source": [
    "# ðŸ¤” Do We Really Need to Rebuild the Model to Run It?\n",
    "\n",
    "That's an excellent question! And the answer is **yes, in this case, we do**. Here's why:\n",
    "\n",
    "When we saved our model in the first notebook, we used `torch.save(model.state_dict(), ...)`. This method saves only the model's **parameters** (the learned weights and biases) in a dictionary. It **does not** save the model's architectureâ€”the Python code that defines the layers (`Conv2d`, `Linear`, etc.) and the `forward` pass.\n",
    "\n",
    "### The \"Scaffold\" Analogy ðŸ—ï¸\n",
    "\n",
    "Think of it like this:\n",
    "1.  The `EmotionCNN` class is the **blueprint** for a building.\n",
    "2.  `model = EmotionCNN()` creates an empty **scaffold** or skeleton of that building.\n",
    "3.  `model.load_state_dict(...)` takes the saved materials (the weights) and **fills in the scaffold**.\n",
    "\n",
    "Without the blueprint (`EmotionCNN` class), PyTorch wouldn't know where to put the saved weights.\n",
    "\n",
    "### Why is this the recommended way?\n",
    "\n",
    "-   **Portability & Safety**: It's the most robust and recommended method. It decouples the model's weights from its code, making it safer and easier to share and load across different projects without running potentially malicious code.\n",
    "-   **Flexibility**: It allows you to modify the model's architecture in your code and still load compatible weights from older versions.\n",
    "\n",
    "So, redefining the class here isn't \"rebuilding\" the model from scratch; it's simply providing the necessary structure for PyTorch to load our pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957babcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully from emotion_model.pth\n",
      "ðŸ“¥ Downloading face detector model: haarcascade_frontalface_default.xml...\n",
      "âœ… Download complete.\n",
      "âœ… Face detector loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE TRAINED MODEL AND FACE DETECTOR\n",
    "# =============================================================================\n",
    "\n",
    "# --- Load Emotion Recognition Model ---\n",
    "MODEL_PATH = \"emotion_model.pth\"\n",
    "\n",
    "\n",
    "def load_emotion_model(model_path):\n",
    "    \"\"\"Load the trained emotion recognition model\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"âŒ Error: Model file not found at {model_path}\")\n",
    "        print(\n",
    "            \"Please make sure you have run the training notebook (01_full_of_emotion.ipynb) first!\"\n",
    "        )\n",
    "        return None, None\n",
    "\n",
    "    # Set weights_only=False because the checkpoint contains non-tensor data (like accuracy)\n",
    "    # This is safe as we trust the source of the model file.\n",
    "    checkpoint = torch.load(\n",
    "        model_path, map_location=torch.device(\"cpu\"), weights_only=False\n",
    "    )\n",
    "\n",
    "    # Create model instance\n",
    "    model = EmotionCNN(num_classes=checkpoint[\"num_classes\"])\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"âœ… Model loaded successfully from {model_path}\")\n",
    "    return model, checkpoint[\"emotion_names\"]\n",
    "\n",
    "\n",
    "model, emotion_names = load_emotion_model(MODEL_PATH)\n",
    "\n",
    "# --- Load Face Detector (Haar Cascade) ---\n",
    "HAAR_CASCADE_URL = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "HAAR_CASCADE_FILENAME = \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.exists(HAAR_CASCADE_FILENAME):\n",
    "    print(f\"ðŸ“¥ Downloading face detector model: {HAAR_CASCADE_FILENAME}...\")\n",
    "    response = requests.get(HAAR_CASCADE_URL)\n",
    "    if response.status_code == 200:\n",
    "        with open(HAAR_CASCADE_FILENAME, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"âœ… Download complete.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"âŒ Failed to download face detector. Status code: {response.status_code}\"\n",
    "        )\n",
    "        face_cascade = None\n",
    "\n",
    "if os.path.exists(HAAR_CASCADE_FILENAME):\n",
    "    face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_FILENAME)\n",
    "    print(f\"âœ… Face detector loaded successfully.\")\n",
    "else:\n",
    "    face_cascade = None\n",
    "    print(\"âŒ Face detector could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ac565",
   "metadata": {},
   "source": [
    "# ðŸ¤” Should We Use a Larger Bounding Box?\n",
    "\n",
    "You noticed that the face detection box can be quite tight around the face. That's a great observation! There are pros and cons to expanding it.\n",
    "\n",
    "### The Argument for a **Tighter** Box (What we had before)\n",
    "-   **Consistency with Training Data**: Our model was trained on the FER-2013 dataset, which contains tightly cropped 48x48 images of faces. By feeding it a similarly tight crop, we are giving it input that most closely matches what it learned from.\n",
    "-   **Reduces Noise**: A tight crop eliminates distracting background elements, clothing, or other objects that are not relevant to the facial expression.\n",
    "\n",
    "### The Argument for a **Wider** Box (What we'll do now)\n",
    "-   **More Context**: A slightly larger box might capture more of the head and hairline, which can sometimes provide subtle cues about an expression.\n",
    "-   **Better Visuals**: A padded box often looks more natural and less claustrophobic on the screen.\n",
    "-   **Robustness**: It can help if the initial face detection is slightly off-center, ensuring the full face is still captured.\n",
    "\n",
    "### The Verdict? Let's Experiment!\n",
    "\n",
    "The only way to know for sure is to try it. We'll add a `padding` factor to the code below. This will expand the area we crop for the model and the box we draw on the screen. You can easily adjust this factor to see how it impacts both the visual result and the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097f5458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting webcam feed... Press 'q' to quit.\n",
      "âœ… Webcam feed stopped.\n",
      "âœ… Webcam feed stopped.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# REAL-TIME EMOTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def predict_emotion(model, image_array, emotion_names):\n",
    "    \"\"\"\n",
    "    Predict emotion from a face image array (grayscale, 48x48)\n",
    "    \"\"\"\n",
    "    # Preprocess image: Convert to tensor, normalize\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]\n",
    "    )\n",
    "\n",
    "    image = Image.fromarray(image_array)\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        pred_idx = torch.argmax(output, dim=1).item()\n",
    "        confidence = probabilities[0][pred_idx].item()\n",
    "\n",
    "    return emotion_names[pred_idx], confidence\n",
    "\n",
    "\n",
    "# --- Main Loop ---\n",
    "if model is not None and face_cascade is not None:\n",
    "    cap = cv2.VideoCapture(0)  # 0 is the default webcam\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"âŒ Error: Could not open webcam.\")\n",
    "    else:\n",
    "        print(\"ðŸš€ Starting webcam feed... Press 'q' to quit.\")\n",
    "\n",
    "        while True:\n",
    "            # Read frame from webcam\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert to grayscale for face detection\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "            )\n",
    "\n",
    "            # Define a padding factor to expand the bounding box (e.g., 0.2 = 20% padding)\n",
    "            padding_factor = 0.2\n",
    "\n",
    "            # Process each detected face\n",
    "            for x, y, w, h in faces:\n",
    "                # Calculate padding\n",
    "                pad_w = int(w * padding_factor)\n",
    "                pad_h = int(h * padding_factor)\n",
    "\n",
    "                # Calculate new, padded coordinates, ensuring they are within frame bounds\n",
    "                x1 = max(0, x - pad_w // 2)\n",
    "                y1 = max(0, y - pad_h // 2)\n",
    "                x2 = min(frame.shape[1], x + w + pad_w // 2)\n",
    "                y2 = min(frame.shape[0], y + h + pad_h // 2)\n",
    "\n",
    "                # Extract the face ROI (Region of Interest) with padding\n",
    "                face_roi = gray_frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Skip if the ROI is empty (can happen at the edge of the screen)\n",
    "                if face_roi.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Resize to 48x48 for the model\n",
    "                resized_face = cv2.resize(face_roi, (48, 48))\n",
    "\n",
    "                # Predict emotion\n",
    "                emotion, confidence = predict_emotion(\n",
    "                    model, resized_face, emotion_names\n",
    "                )\n",
    "\n",
    "                # --- Display results on the frame ---\n",
    "                # Draw the padded rectangle around the face\n",
    "                cv2.rectangle(\n",
    "                    frame, (x1, y1), (x2, y2), (255, 0, 0), 2\n",
    "                )  # Blue rectangle\n",
    "\n",
    "                # Create text for display\n",
    "                label = f\"{emotion} ({confidence:.2f})\"\n",
    "\n",
    "                # Put text above the rectangle\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    label,\n",
    "                    (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.9,\n",
    "                    (255, 0, 0),\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow(\"Real-Time Emotion Detection\", frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"âœ… Webcam feed stopped.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping real-time detection due to missing model or face detector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c95ea9",
   "metadata": {},
   "source": [
    "> **Note on Display:** The `cv2.imshow()` function will open a new window to display the webcam feed. This is normal behavior. To close the feed, make sure the new window is active and press the 'q' key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4902e1",
   "metadata": {},
   "source": [
    "# ðŸ”Œ Game Integration: What's Next?\n",
    "\n",
    "We now have a live emotion label! The final step is to send this information to our game engine.\n",
    "\n",
    "**Current Output:**\n",
    "- `emotion` (string): e.g., \"Happy\", \"Sad\", \"Angry\"\n",
    "- `confidence` (float): e.g., 0.95\n",
    "\n",
    "## ðŸ“¡ Communication Methods\n",
    "\n",
    "How can the game receive this data from our Python script?\n",
    "\n",
    "1.  **Local Sockets (Recommended):**\n",
    "    - Our Python script acts as a **server**.\n",
    "    - The game acts as a **client**.\n",
    "    - The script sends the emotion string over a local network connection (e.g., `localhost:12345`).\n",
    "    - **Pros:** Real-time, low latency.\n",
    "    - **Cons:** Requires network programming in the game engine.\n",
    "\n",
    "2.  **Writing to a File:**\n",
    "    - The Python script continuously writes the current emotion to a text file (e.g., `emotion.txt`).\n",
    "    - The game continuously reads this file.\n",
    "    - **Pros:** Very simple to implement.\n",
    "    - **Cons:** Slower, potential for file access conflicts (less robust).\n",
    "\n",
    "3.  **Web Server (Advanced):**\n",
    "    - The Python script runs a lightweight web server (e.g., using Flask).\n",
    "    - The game makes HTTP requests to get the latest emotion.\n",
    "    - **Pros:** Flexible, can be used remotely.\n",
    "    - **Cons:** More complex setup.\n",
    "\n",
    "**For our project, using a simple socket connection is the most balanced approach for real-time performance and moderate implementation effort.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
