{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/UFResearchComputing/gatorAI_summer_camp_2024/blob/main/01_full_of_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><img src=\"images/gator_ai_camp_2024_logo_200.png\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KhpJb5VI79P"
   },
   "source": [
    "# Gator AI Summer Camp 2025\n",
    "\n",
    "In this notebook, we're going to use Python to create a deep learning model that can take images of faces and output the emotion being expressed.\n",
    "\n",
    "The dataset we're going to use is the FER-2013 dataset, which contains 35,887 grayscale images of faces. Each image is 48x48 pixels and is labeled with one of seven emotions: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset and more information can be found [on Kaggle](https://www.kaggle.com/datasets/msambare/fer2013/data).\n",
    "\n",
    "**Note:** One issue with the dataset is that it has relatively few images in the disgust category, so we drop that category for this exercise.\n",
    "\n",
    "To build our model, we'll use the Keras deep learning library, which provides a high-level interface for building and training neural networks. We'll start by loading the dataset and exploring the images, then we'll build and train a convolutional neural network (CNN) to classify the emotions in the images.\n",
    "\n",
    "**Before you get started, make sure to select a Runtime with a GPU!** <img src='images/colab_change_runtime_type.png' align='right' width='50%' alt='Image of the Runtime menu options in Google Colab'>\n",
    "* Go to the **\"Runtime\"** menu\n",
    "* Select **\"Change runtime type\"**\n",
    "* Select **\"T4 GPU\"** and click **\"Save\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Learning Objectives & What You'll Build\n",
    "\n",
    "## ðŸ§  What is Computer Vision?\n",
    "Computer Vision is a field of AI that teaches computers to \"see\" and understand images, just like humans do! In this notebook, you'll build a system that can look at a person's face and automatically detect their emotion.\n",
    "\n",
    "## ðŸŽ® Real-World Application: Emotion-Aware Gaming\n",
    "The emotion recognition model you'll create will be integrated into our adventure game, allowing Non-Player Characters (NPCs) to respond differently based on your facial expressions. Imagine:\n",
    "- **Sad expression** â†’ NPCs offer comfort and help\n",
    "- **Happy expression** â†’ NPCs share in your joy and give bonuses  \n",
    "- **Angry expression** â†’ NPCs try to calm you down\n",
    "- **Surprised expression** â†’ NPCs react to your amazement\n",
    "\n",
    "## ðŸ“š What You'll Learn Today\n",
    "\n",
    "### ðŸ”¬ **Computer Vision Concepts**\n",
    "- How computers \"see\" and process images\n",
    "- What makes facial expressions recognizable\n",
    "- Image preprocessing and data augmentation\n",
    "\n",
    "### ðŸ§  **Deep Learning Fundamentals**\n",
    "- **Convolutional Neural Networks (CNNs)** - the AI architecture that powers image recognition\n",
    "- **Training Process** - how AI learns from thousands of examples\n",
    "- **Model Evaluation** - measuring how well our AI performs\n",
    "\n",
    "### ðŸ› ï¸ **Practical Skills**\n",
    "- Using **PyTorch Lightning** for efficient deep learning\n",
    "- Working with real-world datasets (FER-2013 emotion dataset)\n",
    "- Visualizing model performance and debugging\n",
    "- Saving and loading trained models for deployment\n",
    "\n",
    "### ðŸŽ® **Game Integration**\n",
    "- Loading pre-trained models in applications\n",
    "- Real-time emotion detection from camera input\n",
    "- Creating responsive NPC behavior based on emotions\n",
    "\n",
    "## ðŸ—ºï¸ Our Journey Today\n",
    "\n",
    "1. **ðŸ“Š Data Exploration** - Understand our emotion dataset\n",
    "2. **ðŸ—ï¸ Model Architecture** - Build our CNN emotion detector  \n",
    "3. **ðŸŽ“ Training Process** - Teach our AI to recognize emotions\n",
    "4. **ðŸ“ˆ Evaluation** - Test how well our model performs\n",
    "5. **ðŸ’¾ Model Saving** - Prepare our model for the game\n",
    "6. **ðŸŽ® Game Integration** - See how it works in practice\n",
    "\n",
    "## ðŸš€ By the End of This Notebook\n",
    "\n",
    "You'll have created a complete emotion recognition system that can:\n",
    "- âœ… Detect 6 different emotions from facial expressions\n",
    "- âœ… Work in real-time with camera input\n",
    "- âœ… Integrate seamlessly with our adventure game\n",
    "- âœ… Provide the foundation for emotion-aware applications\n",
    "\n",
    "**Let's build the future of emotionally intelligent technology!** ðŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVz8_6AYI79Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES: The tools we need to build our emotion recognition system\n",
    "# =============================================================================\n",
    "\n",
    "# Basic Python libraries for file handling and data manipulation\n",
    "import os                    # For working with files and directories\n",
    "import sys                   # For system-specific operations\n",
    "import shutil                # For copying and moving files\n",
    "import zipfile               # For extracting zip archives\n",
    "import random                # For generating random numbers\n",
    "import pandas as pd          # For handling data in table format\n",
    "import numpy as np           # For mathematical operations and arrays\n",
    "import matplotlib.pyplot as plt  # For creating graphs and visualizations\n",
    "from tqdm.auto import tqdm  # Progress bar library\n",
    "import time\n",
    "\n",
    "# Display plots directly in the notebook\n",
    "%matplotlib inline           \n",
    "\n",
    "# Additional utilities\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import kagglehub            # For downloading datasets from Kaggle\n",
    "from PIL import Image # Import PIL Image for reliable image loading\n",
    "\n",
    "# =============================================================================\n",
    "# PYTORCH LIGHTNING: Our main deep learning framework\n",
    "# =============================================================================\n",
    "# PyTorch Lightning makes it easier to organize and train neural networks\n",
    "# It handles a lot of the complex training logic for us!\n",
    "\n",
    "import torch                           # Core PyTorch library\n",
    "import torch.nn as nn                  # Neural network building blocks\n",
    "import torch.nn.functional as F        # Common neural network functions\n",
    "import torchvision                     # Computer vision utilities\n",
    "import torchvision.transforms as transforms  # Image preprocessing tools\n",
    "from torch.utils.data import DataLoader, Dataset  # Data loading utilities\n",
    "\n",
    "import pytorch_lightning as pl         # Lightning framework for easier training\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Import torchmetrics for accuracy calculation\n",
    "import torchmetrics\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION TOOLS: How we measure our model's performance\n",
    "# =============================================================================\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ðŸ§  Ready to build an emotion recognition system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SMART DATA DOWNLOAD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ” Checking if emotion dataset already exists...\")\n",
    "\n",
    "# Check if we already have the data organized properly\n",
    "data_exists = False\n",
    "if os.path.exists(\"data\"):\n",
    "    train_dir = os.path.join(\"data\", \"train\")\n",
    "    test_dir = os.path.join(\"data\", \"test\")\n",
    "\n",
    "    if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
    "        # Check if we have emotion categories in both directories\n",
    "        train_emotions = [\n",
    "            d\n",
    "            for d in os.listdir(train_dir)\n",
    "            if os.path.isdir(os.path.join(train_dir, d))\n",
    "        ]\n",
    "        test_emotions = [\n",
    "            d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            len(train_emotions) >= 5 and len(test_emotions) >= 5\n",
    "        ):  # Should have at least 5 emotion categories\n",
    "            print(\"âœ… Dataset already exists and looks complete!\")\n",
    "            print(f\"   Train emotions: {train_emotions}\")\n",
    "            print(f\"   Test emotions: {test_emotions}\")\n",
    "            data_exists = True\n",
    "        else:\n",
    "            print(\"âš ï¸  Data directory exists but seems incomplete\")\n",
    "            print(f\"   Train emotions found: {train_emotions}\")\n",
    "            print(f\"   Test emotions found: {test_emotions}\")\n",
    "\n",
    "if not data_exists:\n",
    "    print(\"ðŸ“¥ Dataset not found or incomplete. Downloading from Kaggle...\")\n",
    "\n",
    "    # Download the dataset using kagglehub\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    dataset_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "    # Create data directory if it doesn't exist\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "        print(\"Created 'data' directory\")\n",
    "\n",
    "    # Check what files/folders are in the downloaded dataset\n",
    "    print(f\"Contents of {dataset_path}:\")\n",
    "    for item in os.listdir(dataset_path):\n",
    "        item_path = os.path.join(dataset_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  Directory: {item}\")\n",
    "        else:\n",
    "            print(f\"  File: {item}\")\n",
    "\n",
    "    # Look for zip file first\n",
    "    zip_file = None\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".zip\"):\n",
    "            zip_file = os.path.join(dataset_path, file)\n",
    "            break\n",
    "\n",
    "    if zip_file:\n",
    "        # Extract the zip file to the data directory\n",
    "        print(f\"Extracting {zip_file} to data/\")\n",
    "        with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/\")\n",
    "        print(\"Extraction complete\")\n",
    "    else:\n",
    "        # No zip file found, check if train/test directories already exist\n",
    "        train_dir = os.path.join(dataset_path, \"train\")\n",
    "        test_dir = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "        if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
    "            print(\"Found train and test directories, copying to data/\")\n",
    "            # Copy the train and test directories\n",
    "            shutil.copytree(\n",
    "                train_dir, os.path.join(\"data\", \"train\"), dirs_exist_ok=True\n",
    "            )\n",
    "            shutil.copytree(test_dir, os.path.join(\"data\", \"test\"), dirs_exist_ok=True)\n",
    "            print(\"Directories copied successfully\")\n",
    "        else:\n",
    "            # Look for any other structure\n",
    "            print(\"Searching for dataset files in subdirectories...\")\n",
    "            for root, dirs, files in os.walk(dataset_path):\n",
    "                if \"train\" in dirs and \"test\" in dirs:\n",
    "                    train_source = os.path.join(root, \"train\")\n",
    "                    test_source = os.path.join(root, \"test\")\n",
    "                    print(f\"Found train/test directories in: {root}\")\n",
    "                    shutil.copytree(\n",
    "                        train_source, os.path.join(\"data\", \"train\"), dirs_exist_ok=True\n",
    "                    )\n",
    "                    shutil.copytree(\n",
    "                        test_source, os.path.join(\"data\", \"test\"), dirs_exist_ok=True\n",
    "                    )\n",
    "                    print(\"Directories copied successfully\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Could not find train/test directories in the downloaded dataset\")\n",
    "\n",
    "# Verify the final data directory structure\n",
    "if os.path.exists(\"data\"):\n",
    "    print(f\"\\nðŸ“ Final data directory structure:\")\n",
    "    for item in os.listdir(\"data\"):\n",
    "        item_path = os.path.join(\"data\", item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  Directory: {item}\")\n",
    "            # Show subdirectories (emotion categories) and count files\n",
    "            if os.path.exists(item_path):\n",
    "                subdirs = [\n",
    "                    d\n",
    "                    for d in os.listdir(item_path)\n",
    "                    if os.path.isdir(os.path.join(item_path, d))\n",
    "                ]\n",
    "                print(f\"    Emotion categories: {subdirs}\")\n",
    "\n",
    "                # Count total images\n",
    "                total_images = 0\n",
    "                for emotion_dir in subdirs:\n",
    "                    emotion_path = os.path.join(item_path, emotion_dir)\n",
    "                    image_files = [\n",
    "                        f\n",
    "                        for f in os.listdir(emotion_path)\n",
    "                        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "                    ]\n",
    "                    total_images += len(image_files)\n",
    "                print(f\"    Total images: {total_images}\")\n",
    "\n",
    "# Clean up disgust category (only if data was just downloaded or if disgust still exists)\n",
    "disgust_train_path = os.path.join(\"data\", \"train\", \"disgust\")\n",
    "disgust_test_path = os.path.join(\"data\", \"test\", \"disgust\")\n",
    "\n",
    "disgust_removed = False\n",
    "if os.path.exists(disgust_train_path):\n",
    "    shutil.rmtree(disgust_train_path)\n",
    "    print(\"\\nðŸ—‘ï¸  Removed data/train/disgust folder (too few examples)\")\n",
    "    disgust_removed = True\n",
    "\n",
    "if os.path.exists(disgust_test_path):\n",
    "    shutil.rmtree(disgust_test_path)\n",
    "    print(\"ðŸ—‘ï¸  Removed data/test/disgust folder (too few examples)\")\n",
    "    disgust_removed = True\n",
    "\n",
    "if not disgust_removed:\n",
    "    print(\"\\nâœ… Disgust category already removed or not present\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ” Understanding Our Dataset & Data Augmentation\n",
    "\n",
    "## ðŸ“Š What We're Working With\n",
    "Our emotion dataset contains grayscale face images (48x48 pixels) across 6 emotion categories:\n",
    "- **Angry** ðŸ˜  - Furrowed brows, tense facial muscles\n",
    "- **Fear** ðŸ˜¨ - Wide eyes, open mouth\n",
    "- **Happy** ðŸ˜Š - Smiling, raised cheeks\n",
    "- **Neutral** ðŸ˜ - Relaxed, no strong expression  \n",
    "- **Sad** ðŸ˜¢ - Downturned mouth, drooping eyes\n",
    "- **Surprise** ðŸ˜® - Raised eyebrows, wide eyes\n",
    "\n",
    "## ðŸŽ¨ Data Augmentation: Teaching AI to See Better\n",
    "Data augmentation is like showing our AI the same face from different angles and lighting conditions. This helps it become more robust and generalize better to new faces it hasn't seen before.\n",
    "\n",
    "**Key Augmentation Techniques:**\n",
    "- **Random Horizontal Flip** - People can face left or right\n",
    "- **Random Rotation** - Slight head tilts are natural\n",
    "- **Random Brightness/Contrast** - Different lighting conditions\n",
    "- **Random Noise** - Real-world images aren't perfect\n",
    "\n",
    "Think of it like this: If you only practiced recognizing happy faces from photos taken in perfect lighting, you might struggle to recognize happiness in a dimly lit room. Data augmentation prevents this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM DATASET WITH DATA AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Custom dataset for emotion recognition with built-in augmentation\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Emotion categories (excluding disgust as mentioned in notebook)\n",
    "        self.emotions = [\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "        self.emotion_to_idx = {\n",
    "            emotion: idx for idx, emotion in enumerate(self.emotions)\n",
    "        }\n",
    "\n",
    "        # Load all image paths and labels\n",
    "        for emotion in self.emotions:\n",
    "            emotion_dir = os.path.join(data_dir, emotion)\n",
    "            if os.path.exists(emotion_dir):\n",
    "                for img_file in os.listdir(emotion_dir):\n",
    "                    if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                        self.images.append(os.path.join(emotion_dir, img_file))\n",
    "                        self.labels.append(self.emotion_to_idx[emotion])\n",
    "\n",
    "        print(f\"ðŸ“Š Loaded {len(self.images)} images from {data_dir}\")\n",
    "\n",
    "        # Print class distribution\n",
    "        label_counts = Counter(self.labels)\n",
    "        for emotion, idx in self.emotion_to_idx.items():\n",
    "            count = label_counts.get(idx, 0)\n",
    "            print(f\"   {emotion}: {count} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.images[idx]\n",
    "        try:\n",
    "            # Use PIL to ensure consistent loading\n",
    "            image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "            image = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            image = np.zeros((48, 48), dtype=np.uint8)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to tensor and apply transforms\n",
    "        if self.transform:\n",
    "            # Convert to PIL for transforms, then back to tensor\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Basic conversion to tensor\n",
    "            image = torch.FloatTensor(image).unsqueeze(0) / 255.0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA AUGMENTATION TRANSFORMS\n",
    "# =============================================================================\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip\n",
    "        transforms.RandomRotation(degrees=10),  # Rotate up to 10 degrees\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2\n",
    "        ),  # Vary brightness/contrast\n",
    "        transforms.ToTensor(),  # Convert to tensor [0,1]\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize to [-1,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation/test transforms (no augmentation)\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ… Dataset class and transforms defined!\")\n",
    "print(\"ðŸŽ¨ Training uses data augmentation for better generalization\")\n",
    "print(\"ðŸ“ Validation uses clean transforms for accurate evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—ï¸ Building Our Compact CNN Model\n",
    "\n",
    "## ðŸ§  Why Convolutional Neural Networks (CNNs)?\n",
    "CNNs are perfect for image recognition because they mimic how our visual cortex works:\n",
    "\n",
    "1. **Convolutional Layers** ðŸ” - Act like filters that detect features (edges, shapes, patterns)\n",
    "2. **Pooling Layers** ðŸ“‰ - Reduce image size while keeping important information  \n",
    "3. **Dense Layers** ðŸ§® - Make final decisions based on detected features\n",
    "\n",
    "## ðŸ“ Our Model Architecture (â‰ˆ6M Parameters)\n",
    "```\n",
    "Input: 48x48 grayscale image\n",
    "â”œâ”€â”€ Conv Block 1: 32 filters â†’ Feature maps\n",
    "â”œâ”€â”€ Conv Block 2: 64 filters â†’ More complex features  \n",
    "â”œâ”€â”€ Conv Block 3: 128 filters â†’ High-level patterns\n",
    "â”œâ”€â”€ Global Average Pooling â†’ Efficient dimensionality reduction\n",
    "â”œâ”€â”€ Dense Layer: 128 units â†’ Final feature processing\n",
    "â””â”€â”€ Output: 6 emotions (softmax probabilities)\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ Design Principles\n",
    "- **Efficient**: Uses Global Average Pooling instead of large dense layers\n",
    "- **Robust**: Batch normalization and dropout prevent overfitting\n",
    "- **Compact**: Carefully balanced to stay around 6M parameters\n",
    "- **Modern**: Follows current best practices for CNN design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPACT CNN MODEL (~6M PARAMETERS)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EmotionCNN(pl.LightningModule):\n",
    "    \"\"\"Compact CNN for emotion recognition using PyTorch Lightning\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=6, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Convolutional layers with batch normalization\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 48x48 -> 24x24\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 24x24 -> 12x12\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 12x12 -> 6x6\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling (more efficient than flattening)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # 6x6x128 -> 1x1x128\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "        # Track accuracy using torchmetrics\n",
    "        self.train_acc = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=num_classes\n",
    "        )\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = self.train_acc(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = self.val_acc(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Create model and check parameter count\n",
    "model = EmotionCNN(num_classes=6)\n",
    "param_count = count_parameters(model)\n",
    "print(f\"ðŸ§  Model created with {param_count:,} parameters\")\n",
    "print(\n",
    "    f\"ðŸŽ¯ Target was ~6M parameters - {'âœ… Perfect!' if 5_000_000 <= param_count <= 7_000_000 else 'âš ï¸ Adjust if needed'}\"\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "print(f\"\\nðŸ“‹ Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create datasets\n",
    "print(\"ðŸ“Š Creating datasets...\")\n",
    "train_dataset = EmotionDataset(\"data/train\", transform=train_transforms)\n",
    "test_dataset = EmotionDataset(\"data/test\", transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,  # Faster GPU transfer\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data loaders created!\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "\n",
    "# Visualize a few sample images\n",
    "def show_sample_images(dataset, num_samples=8):\n",
    "    \"\"\"Display sample images from the dataset\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    fig.suptitle(\"Sample Images from Dataset\", fontsize=16)\n",
    "\n",
    "    emotions = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Convert tensor back to displayable format\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            if image.shape[0] == 1:  # Remove channel dimension\n",
    "                image = image.squeeze(0)\n",
    "            # Denormalize if needed\n",
    "            if image.min() < 0:  # Normalized to [-1,1]\n",
    "                image = (image + 1) / 2\n",
    "\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        axes[row, col].imshow(image, cmap=\"gray\")\n",
    "        axes[row, col].set_title(f\"{emotions[label]}\")\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show sample images\n",
    "print(\"ðŸ–¼ï¸ Sample images from the training dataset:\")\n",
    "show_sample_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Training Our Emotion Recognition Model\n",
    "\n",
    "## ðŸ”„ The Training Process\n",
    "Think of training like teaching a child to recognize emotions:\n",
    "\n",
    "1. **Show Examples** ðŸ“š - Feed the model thousands of labeled face images\n",
    "2. **Make Predictions** ðŸ¤” - Model guesses the emotion in each image  \n",
    "3. **Learn from Mistakes** ðŸ“ˆ - Adjust internal parameters when wrong\n",
    "4. **Repeat & Improve** ðŸ” - Continue until the model gets really good\n",
    "\n",
    "## ðŸ“Š What We're Monitoring\n",
    "- **Training Loss** ðŸ“‰ - How confident the model is (lower = better)\n",
    "- **Training Accuracy** ðŸŽ¯ - Percentage of correct predictions on training data\n",
    "- **Validation Accuracy** âœ… - Performance on unseen test data (most important!)\n",
    "\n",
    "## ðŸ›¡ï¸ Preventing Overfitting\n",
    "- **Early Stopping** â¹ï¸ - Stop training when validation performance plateaus\n",
    "- **Dropout** ðŸŽ² - Randomly \"turn off\" neurons during training\n",
    "- **Data Augmentation** ðŸŽ¨ - Show model varied versions of the same image\n",
    "\n",
    "**Goal:** Create a model that works well on faces it has never seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration\n",
    "max_epochs = 30\n",
    "patience = 7  # Stop if no improvement for 7 epochs\n",
    "\n",
    "# Set up callbacks\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=patience, verbose=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    filename=\"emotion-cnn-{epoch:02d}-{val_acc:.3f}\",\n",
    ")\n",
    "\n",
    "# Set up logger for tensorboard (optional)\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"emotion_cnn\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger=logger,\n",
    "    accelerator=\"auto\",  # Automatically use GPU if available\n",
    "    devices=\"auto\",\n",
    "    precision=16,  # Use mixed precision for faster training\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize fresh model\n",
    "model = EmotionCNN(num_classes=6, learning_rate=0.001)\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(f\"   Max epochs: {max_epochs}\")\n",
    "print(f\"   Early stopping patience: {patience}\")\n",
    "print(f\"   Using device: {trainer.accelerator}\")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Training completed!\")\n",
    "print(f\"   Total training time: {training_time:.1f} seconds\")\n",
    "print(f\"   Best model saved at: {checkpoint_callback.best_model_path}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = EmotionCNN.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "print(\n",
    "    f\"âœ… Best model loaded with validation accuracy: {checkpoint_callback.best_model_score:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL EVALUATION AND VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, emotion_names):\n",
    "    \"\"\"Evaluate model and create visualizations\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"ðŸ“Š Evaluating model on test set...\")\n",
    "\n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"   Model device: {device}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images, labels = batch\n",
    "            # Move data to the same device as the model\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Final Test Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nðŸ“ˆ Detailed Performance Report:\")\n",
    "    report = classification_report(all_labels, all_preds, target_names=emotion_names)\n",
    "    print(report)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Emotion Recognition\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(emotion_names))\n",
    "    plt.xticks(tick_marks, emotion_names, rotation=45)\n",
    "    plt.yticks(tick_marks, emotion_names)\n",
    "\n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], \"d\"),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "def show_predictions(model, test_dataset, num_samples=8):\n",
    "    \"\"\"Show model predictions on sample images\"\"\"\n",
    "    model.eval()\n",
    "    emotion_names = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    fig.suptitle(\"Model Predictions on Test Images\", fontsize=16)\n",
    "\n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            idx = random.randint(0, len(test_dataset) - 1)\n",
    "            image, true_label = test_dataset[idx]\n",
    "\n",
    "            # Get model prediction - move image to same device as model\n",
    "            image_input = image.unsqueeze(0).to(\n",
    "                device\n",
    "            )  # Add batch dimension and move to device\n",
    "            output = model(image_input)\n",
    "            pred_prob = F.softmax(output, dim=1)\n",
    "            pred_label = torch.argmax(output, dim=1).item()\n",
    "            confidence = pred_prob[0][pred_label].item()\n",
    "\n",
    "            # Display image (keep on CPU for matplotlib)\n",
    "            display_img = image.squeeze(0) if image.shape[0] == 1 else image\n",
    "            if display_img.min() < 0:  # Denormalize if needed\n",
    "                display_img = (display_img + 1) / 2\n",
    "\n",
    "            row = i // 4\n",
    "            col = i % 4\n",
    "            axes[row, col].imshow(display_img, cmap=\"gray\")\n",
    "\n",
    "            # Create title with prediction\n",
    "            true_emotion = emotion_names[true_label]\n",
    "            pred_emotion = emotion_names[pred_label]\n",
    "            color = \"green\" if true_label == pred_label else \"red\"\n",
    "\n",
    "            title = f\"True: {true_emotion}\\nPred: {pred_emotion} ({confidence:.2f})\"\n",
    "            axes[row, col].set_title(title, color=color, fontsize=10)\n",
    "            axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Move model to CPU for evaluation (ensures compatibility)\n",
    "print(\"ðŸ”§ Moving model to CPU for evaluation...\")\n",
    "best_model = best_model.cpu()\n",
    "\n",
    "# Evaluate the model\n",
    "emotion_names = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "test_accuracy, predictions, true_labels = evaluate_model(\n",
    "    best_model, test_loader, emotion_names\n",
    ")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nðŸ” Sample Predictions (Green=Correct, Red=Incorrect):\")\n",
    "show_predictions(best_model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ Transfer Learning: Standing on the Shoulders of Giants\n",
    "\n",
    "## ðŸ¤” What is Transfer Learning?\n",
    "Transfer learning is like learning to drive a motorcycle after you already know how to ride a bicycle - you leverage existing knowledge to learn something new faster!\n",
    "\n",
    "In deep learning, we take a model that has already been trained on millions of images (like ImageNet) and adapt it for our specific task. This is incredibly powerful because:\n",
    "\n",
    "## ðŸŒŸ **Benefits of Transfer Learning**\n",
    "- **Faster Training** âš¡ - No need to learn basic features from scratch\n",
    "- **Better Performance** ðŸ“ˆ - Pre-trained models already know edges, shapes, and patterns\n",
    "- **Less Data Required** ðŸ’¾ - Works well even with smaller datasets\n",
    "- **Proven Architecture** ðŸ—ï¸ - Uses battle-tested model designs\n",
    "\n",
    "## ðŸ§  **How It Works**\n",
    "1. **Take a pre-trained model** (trained on ImageNet with millions of images)\n",
    "2. **Remove the final layer** (originally for 1000 classes)\n",
    "3. **Add new layers** specific to our 6 emotions\n",
    "4. **Fine-tune** the entire network or just the new layers\n",
    "\n",
    "## ðŸ”¬ **Our Experiment**\n",
    "We'll use **ResNet18** - a proven architecture that won ImageNet competitions. Let's see how it compares to our custom CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRANSFER LEARNING MODEL (RESNET18)\n",
    "# =============================================================================\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class TransferLearningCNN(pl.LightningModule):\n",
    "    \"\"\"Transfer Learning CNN using pre-trained ResNet18\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=6, learning_rate=0.001, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "\n",
    "        # Load pre-trained ResNet18\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Modify first layer to accept grayscale images (1 channel instead of 3)\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "\n",
    "        # Get the number of features from the last layer\n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        # Replace the final fully connected layer\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "        # Optionally freeze the backbone (only train the new classifier)\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze the new layers we added\n",
    "            for param in self.backbone.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Unfreeze the modified conv1 layer\n",
    "            for param in self.backbone.conv1.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Track accuracy using torchmetrics\n",
    "        self.train_acc = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=num_classes\n",
    "        )\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = self.train_acc(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = self.val_acc(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use different learning rates for backbone vs new layers\n",
    "        if self.freeze_backbone:\n",
    "            # Only optimize the new classifier layers\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": self.backbone.fc.parameters()},\n",
    "                    {\"params\": self.backbone.conv1.parameters()},\n",
    "                ],\n",
    "                lr=self.learning_rate,\n",
    "            )\n",
    "        else:\n",
    "            # Optimize all parameters, but use lower LR for pre-trained parts\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": self.backbone.conv1.parameters(),\n",
    "                        \"lr\": self.learning_rate,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": self.backbone.layer1.parameters(),\n",
    "                        \"lr\": self.learning_rate * 0.1,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": self.backbone.layer2.parameters(),\n",
    "                        \"lr\": self.learning_rate * 0.1,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": self.backbone.layer3.parameters(),\n",
    "                        \"lr\": self.learning_rate * 0.1,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": self.backbone.layer4.parameters(),\n",
    "                        \"lr\": self.learning_rate * 0.1,\n",
    "                    },\n",
    "                    {\"params\": self.backbone.fc.parameters(), \"lr\": self.learning_rate},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Create and analyze the transfer learning model\n",
    "transfer_model = TransferLearningCNN(num_classes=6, freeze_backbone=False)\n",
    "transfer_param_count = count_parameters(transfer_model)\n",
    "\n",
    "print(f\"ðŸ”„ Transfer Learning Model (ResNet18) created!\")\n",
    "print(f\"   Parameters: {transfer_param_count:,}\")\n",
    "print(f\"   Pre-trained: âœ… (ImageNet weights)\")\n",
    "print(f\"   Modified for: 1-channel input (grayscale)\")\n",
    "print(f\"   Custom classifier: 6 emotion classes\")\n",
    "\n",
    "# Compare model sizes\n",
    "print(f\"\\nðŸ“Š Model Comparison:\")\n",
    "print(f\"   Custom CNN: {count_parameters(model):,} parameters\")\n",
    "print(f\"   Transfer Learning: {transfer_param_count:,} parameters\")\n",
    "print(\n",
    "    f\"   Difference: {abs(transfer_param_count - count_parameters(model)):,} parameters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN THE TRANSFER LEARNING MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration for transfer learning (usually needs fewer epochs)\n",
    "max_epochs_transfer = 15  # Transfer learning often converges faster\n",
    "patience_transfer = 5\n",
    "\n",
    "# Set up callbacks for transfer learning\n",
    "early_stop_callback_transfer = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=patience_transfer, verbose=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback_transfer = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    filename=\"transfer-emotion-cnn-{epoch:02d}-{val_acc:.3f}\",\n",
    ")\n",
    "\n",
    "# Create trainer for transfer learning\n",
    "trainer_transfer = pl.Trainer(\n",
    "    max_epochs=max_epochs_transfer,\n",
    "    callbacks=[early_stop_callback_transfer, checkpoint_callback_transfer],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\", name=\"transfer_emotion_cnn\"),\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=16,\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize fresh transfer learning model\n",
    "transfer_model = TransferLearningCNN(num_classes=6, learning_rate=0.001)\n",
    "\n",
    "print(\"ðŸš€ Starting transfer learning training...\")\n",
    "print(f\"   Max epochs: {max_epochs_transfer}\")\n",
    "print(f\"   Early stopping patience: {patience_transfer}\")\n",
    "print(f\"   Model: ResNet18 with custom classifier\")\n",
    "\n",
    "# Train the transfer learning model\n",
    "start_time_transfer = time.time()\n",
    "trainer_transfer.fit(transfer_model, train_loader, test_loader)\n",
    "training_time_transfer = time.time() - start_time_transfer\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Transfer learning training completed!\")\n",
    "print(f\"   Total training time: {training_time_transfer:.1f} seconds\")\n",
    "print(f\"   Best model saved at: {checkpoint_callback_transfer.best_model_path}\")\n",
    "\n",
    "# Load the best transfer learning model\n",
    "best_transfer_model = TransferLearningCNN.load_from_checkpoint(\n",
    "    checkpoint_callback_transfer.best_model_path\n",
    ")\n",
    "print(\n",
    "    f\"âœ… Best transfer model loaded with validation accuracy: {checkpoint_callback_transfer.best_model_score:.3f}\"\n",
    ")\n",
    "\n",
    "# Move to CPU for evaluation\n",
    "print(\"ðŸ”§ Moving transfer learning model to CPU for evaluation...\")\n",
    "best_transfer_model = best_transfer_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL COMPARISON: CUSTOM CNN vs TRANSFER LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "# Evaluate the transfer learning model\n",
    "print(\"ðŸ“Š Evaluating Transfer Learning Model...\")\n",
    "transfer_accuracy, transfer_preds, transfer_labels = evaluate_model(\n",
    "    best_transfer_model, test_loader, emotion_names\n",
    ")\n",
    "\n",
    "\n",
    "# Create comprehensive comparison\n",
    "def compare_models():\n",
    "    \"\"\"Compare the two approaches comprehensively\"\"\"\n",
    "\n",
    "    print(\"ðŸ” COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Performance Comparison\n",
    "    print(\"\\nðŸ“ˆ PERFORMANCE METRICS:\")\n",
    "    print(f\"Custom CNN Accuracy:      {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
    "    print(\n",
    "        f\"Transfer Learning Accuracy: {transfer_accuracy:.3f} ({transfer_accuracy*100:.1f}%)\"\n",
    "    )\n",
    "    print(f\"Accuracy Difference:      {abs(transfer_accuracy - test_accuracy):.3f}\")\n",
    "\n",
    "    if transfer_accuracy > test_accuracy:\n",
    "        print(\"ðŸ† Winner: Transfer Learning ResNet18\")\n",
    "    elif test_accuracy > transfer_accuracy:\n",
    "        print(\"ðŸ† Winner: Custom CNN\")\n",
    "    else:\n",
    "        print(\"ðŸ¤ Tie: Both models perform equally!\")\n",
    "\n",
    "    # Model Specifications\n",
    "    print(\"\\nðŸ—ï¸ MODEL SPECIFICATIONS:\")\n",
    "    custom_params = count_parameters(model)\n",
    "    transfer_params = count_parameters(transfer_model)\n",
    "\n",
    "    print(f\"Custom CNN Parameters:    {custom_params:,}\")\n",
    "    print(f\"Transfer Learning Parameters: {transfer_params:,}\")\n",
    "    print(f\"Parameter Difference:     {abs(transfer_params - custom_params):,}\")\n",
    "\n",
    "    # Training Time Comparison\n",
    "    print(f\"\\nâ±ï¸ TRAINING TIME:\")\n",
    "    print(f\"Custom CNN Training:      {training_time:.1f} seconds\")\n",
    "    print(f\"Transfer Learning Training: {training_time_transfer:.1f} seconds\")\n",
    "    print(\n",
    "        f\"Time Difference:          {abs(training_time_transfer - training_time):.1f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Efficiency Analysis\n",
    "    print(f\"\\nâš¡ EFFICIENCY ANALYSIS:\")\n",
    "    custom_efficiency = test_accuracy / (\n",
    "        custom_params / 1_000_000\n",
    "    )  # Accuracy per million params\n",
    "    transfer_efficiency = transfer_accuracy / (transfer_params / 1_000_000)\n",
    "\n",
    "    print(\n",
    "        f\"Custom CNN Efficiency:    {custom_efficiency:.3f} (accuracy per million params)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Transfer Learning Efficiency: {transfer_efficiency:.3f} (accuracy per million params)\"\n",
    "    )\n",
    "\n",
    "    if transfer_efficiency > custom_efficiency:\n",
    "        print(\"ðŸŽ¯ Most Efficient: Transfer Learning\")\n",
    "    else:\n",
    "        print(\"ðŸŽ¯ Most Efficient: Custom CNN\")\n",
    "\n",
    "    # Recommendations\n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    print(\"ðŸ“± For Mobile/Edge Deployment: Custom CNN (smaller, faster)\")\n",
    "    print(\"ðŸŽ¯ For Maximum Accuracy: Transfer Learning (proven architecture)\")\n",
    "    print(\"âš¡ For Quick Prototyping: Transfer Learning (faster training)\")\n",
    "    print(\"ðŸ§  For Learning/Research: Custom CNN (understand from scratch)\")\n",
    "\n",
    "    return {\n",
    "        \"custom_accuracy\": test_accuracy,\n",
    "        \"transfer_accuracy\": transfer_accuracy,\n",
    "        \"custom_params\": custom_params,\n",
    "        \"transfer_params\": transfer_params,\n",
    "        \"custom_time\": training_time,\n",
    "        \"transfer_time\": training_time_transfer,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "comparison_results = compare_models()\n",
    "\n",
    "\n",
    "# Visual comparison of predictions\n",
    "def compare_predictions(custom_model, transfer_model, test_dataset, num_samples=8):\n",
    "    \"\"\"Show side-by-side predictions from both models\"\"\"\n",
    "    custom_model.eval()\n",
    "    transfer_model.eval()\n",
    "    emotion_names = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 6))\n",
    "    fig.suptitle(\"Model Comparison: Custom CNN vs Transfer Learning\", fontsize=16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            idx = random.randint(0, len(test_dataset) - 1)\n",
    "            image, true_label = test_dataset[idx]\n",
    "\n",
    "            # Get predictions from both models\n",
    "            image_input = image.unsqueeze(0)\n",
    "\n",
    "            # Custom CNN prediction\n",
    "            custom_output = custom_model(image_input)\n",
    "            custom_prob = F.softmax(custom_output, dim=1)\n",
    "            custom_pred = torch.argmax(custom_output, dim=1).item()\n",
    "            custom_conf = custom_prob[0][custom_pred].item()\n",
    "\n",
    "            # Transfer Learning prediction\n",
    "            transfer_output = transfer_model(image_input)\n",
    "            transfer_prob = F.softmax(transfer_output, dim=1)\n",
    "            transfer_pred = torch.argmax(transfer_output, dim=1).item()\n",
    "            transfer_conf = transfer_prob[0][transfer_pred].item()\n",
    "\n",
    "            # Display image\n",
    "            display_img = image.squeeze(0)\n",
    "            if display_img.min() < 0:\n",
    "                display_img = (display_img + 1) / 2\n",
    "\n",
    "            # Custom CNN result (top row)\n",
    "            axes[0, i].imshow(display_img, cmap=\"gray\")\n",
    "            custom_color = \"green\" if custom_pred == true_label else \"red\"\n",
    "            custom_title = (\n",
    "                f\"Custom CNN\\n{emotion_names[custom_pred]} ({custom_conf:.2f})\"\n",
    "            )\n",
    "            axes[0, i].set_title(custom_title, color=custom_color, fontsize=10)\n",
    "            axes[0, i].axis(\"off\")\n",
    "\n",
    "            # Transfer Learning result (bottom row)\n",
    "            axes[1, i].imshow(display_img, cmap=\"gray\")\n",
    "            transfer_color = \"green\" if transfer_pred == true_label else \"red\"\n",
    "            transfer_title = f\"Transfer ResNet18\\n{emotion_names[transfer_pred]} ({transfer_conf:.2f})\"\n",
    "            axes[1, i].set_title(transfer_title, color=transfer_color, fontsize=10)\n",
    "            axes[1, i].axis(\"off\")\n",
    "\n",
    "            # Add true label on the left\n",
    "            if i == 0:\n",
    "                axes[0, i].text(\n",
    "                    -0.1,\n",
    "                    0.5,\n",
    "                    \"Custom\\nCNN\",\n",
    "                    transform=axes[0, i].transAxes,\n",
    "                    ha=\"right\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=12,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "                axes[1, i].text(\n",
    "                    -0.1,\n",
    "                    0.5,\n",
    "                    \"Transfer\\nLearning\",\n",
    "                    transform=axes[1, i].transAxes,\n",
    "                    ha=\"right\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=12,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ðŸ” Green = Correct Prediction, Red = Incorrect Prediction\")\n",
    "    print(\"ðŸ“Š Compare how each model performs on the same images!\")\n",
    "\n",
    "\n",
    "# Show side-by-side predictions\n",
    "print(\"\\nðŸŽ¨ Visual Comparison of Model Predictions:\")\n",
    "compare_predictions(best_model, best_transfer_model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ­ Custom CNN vs Transfer Learning: Lessons Learned\n",
    "\n",
    "## ðŸ§  What Did We Discover?\n",
    "\n",
    "### ðŸ—ï¸ **Custom CNN Approach**\n",
    "**Pros:**\n",
    "- âœ… **Lightweight** - Designed specifically for our 48x48 emotion task\n",
    "- âœ… **Educational** - Built from scratch to understand every component\n",
    "- âœ… **Efficient** - Optimized parameter count (~6M parameters)\n",
    "- âœ… **Fast Inference** - Smaller model means faster predictions\n",
    "\n",
    "**Cons:**\n",
    "- âŒ **Limited Experience** - Starts learning features from zero\n",
    "- âŒ **More Training Time** - Needs to learn basic visual patterns\n",
    "- âŒ **Data Hungry** - Requires more examples to learn effectively\n",
    "\n",
    "### ðŸ”„ **Transfer Learning Approach**\n",
    "**Pros:**\n",
    "- âœ… **Pre-trained Knowledge** - Already knows edges, shapes, textures\n",
    "- âœ… **Faster Convergence** - Leverages ImageNet experience\n",
    "- âœ… **Better Generalization** - Proven architecture from millions of images\n",
    "- âœ… **State-of-the-art Base** - Built on research-proven ResNet\n",
    "\n",
    "**Cons:**\n",
    "- âŒ **Larger Model** - More parameters mean more memory/compute\n",
    "- âŒ **Overkill for Simple Tasks** - Might be too complex for basic problems\n",
    "- âŒ **Less Educational** - Harder to understand all components\n",
    "\n",
    "## ðŸ¤” **When to Use Each Approach?**\n",
    "\n",
    "### Choose **Custom CNN** when:\n",
    "- ðŸŽ¯ **Learning Focus** - You want to understand CNNs from scratch\n",
    "- ðŸ“± **Resource Constraints** - Mobile/edge deployment with limited memory\n",
    "- âš¡ **Speed Critical** - Need fastest possible inference\n",
    "- ðŸŽ¨ **Unique Problem** - Very specialized task unlike natural images\n",
    "\n",
    "### Choose **Transfer Learning** when:\n",
    "- ðŸ† **Maximum Performance** - Accuracy is the top priority\n",
    "- â° **Time Constraints** - Need results quickly\n",
    "- ðŸ“Š **Limited Data** - Working with smaller datasets\n",
    "- ðŸ­ **Production Systems** - Building real-world applications\n",
    "\n",
    "## ðŸ’¡ **Key Insights**\n",
    "\n",
    "1. **No Universal Winner** ðŸ¤ - Both approaches have their place\n",
    "2. **Problem Context Matters** ðŸŽ¯ - Choose based on your specific needs\n",
    "3. **Trade-offs Are Real** âš–ï¸ - Size vs accuracy, speed vs performance\n",
    "4. **Experimentation Wins** ðŸ”¬ - Always try both and compare!\n",
    "\n",
    "The beauty of deep learning is having these powerful tools at your disposal. Understanding both approaches makes you a more versatile AI engineer! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL SAVING AND DEPLOYMENT PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# Save the final trained model for use in the game\n",
    "model_save_path = \"emotion_model.pth\"\n",
    "\n",
    "# Save model state dict (lighter weight option)\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": best_model.state_dict(),\n",
    "        \"model_class\": \"EmotionCNN\",\n",
    "        \"num_classes\": 6,\n",
    "        \"emotion_names\": emotion_names,\n",
    "        \"input_size\": (1, 48, 48),\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "    },\n",
    "    model_save_path,\n",
    ")\n",
    "\n",
    "print(f\"ðŸ’¾ Model saved to {model_save_path}\")\n",
    "print(f\"ðŸ“Š Test accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "\n",
    "# Create a simple inference function for the game\n",
    "def load_emotion_model(model_path):\n",
    "    \"\"\"Load the trained emotion recognition model\"\"\"\n",
    "    checkpoint = torch.load(model_path)\n",
    "\n",
    "    # Create model instance\n",
    "    model = EmotionCNN(num_classes=checkpoint[\"num_classes\"])\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    return model, checkpoint[\"emotion_names\"]\n",
    "\n",
    "\n",
    "def predict_emotion(model, image_array, emotion_names):\n",
    "    \"\"\"\n",
    "    Predict emotion from a face image\n",
    "\n",
    "    Args:\n",
    "        model: Trained emotion recognition model\n",
    "        image_array: Grayscale image array (48x48)\n",
    "        emotion_names: List of emotion names\n",
    "\n",
    "    Returns:\n",
    "        emotion: Predicted emotion name\n",
    "        confidence: Confidence score (0-1)\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    image = torch.FloatTensor(image_array).unsqueeze(0).unsqueeze(0) / 255.0\n",
    "    image = (image - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        pred_idx = torch.argmax(output, dim=1).item()\n",
    "        confidence = probabilities[0][pred_idx].item()\n",
    "\n",
    "    return emotion_names[pred_idx], confidence\n",
    "\n",
    "\n",
    "# Test the loading and inference functions\n",
    "print(\"\\nðŸ§ª Testing model loading and inference...\")\n",
    "loaded_model, loaded_emotions = load_emotion_model(model_save_path)\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"   Emotion classes: {loaded_emotions}\")\n",
    "\n",
    "# Test with a random image from the test set\n",
    "test_idx = random.randint(0, len(test_dataset) - 1)\n",
    "test_image, true_label = test_dataset[test_idx]\n",
    "\n",
    "# Convert back to numpy for the inference function\n",
    "test_img_np = test_image.squeeze(0).numpy()\n",
    "if test_img_np.min() < 0:  # Denormalize\n",
    "    test_img_np = (test_img_np + 1) / 2\n",
    "test_img_np = (test_img_np * 255).astype(np.uint8)\n",
    "\n",
    "predicted_emotion, confidence = predict_emotion(\n",
    "    loaded_model, test_img_np, loaded_emotions\n",
    ")\n",
    "true_emotion = emotion_names[true_label]\n",
    "\n",
    "print(f\"\\nðŸ” Inference Test:\")\n",
    "print(f\"   True emotion: {true_emotion}\")\n",
    "print(f\"   Predicted: {predicted_emotion} (confidence: {confidence:.3f})\")\n",
    "print(\n",
    "    f\"   Result: {'âœ… Correct!' if predicted_emotion.lower() == true_emotion.lower() else 'âŒ Incorrect'}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ® Model is ready for game integration!\")\n",
    "print(f\"   Model file: {model_save_path}\")\n",
    "print(f\"   Use load_emotion_model() and predict_emotion() functions in your game\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Congratulations! You've Built an Emotion Recognition System!\n",
    "\n",
    "## ðŸ† What You've Accomplished\n",
    "\n",
    "### ðŸ§  **Computer Vision Mastery**\n",
    "- âœ… Built a **Convolutional Neural Network** with ~6M parameters\n",
    "- âœ… Implemented **data augmentation** for robust training\n",
    "- âœ… Achieved emotion recognition across 6 different emotions\n",
    "- âœ… Created a complete training and evaluation pipeline\n",
    "\n",
    "### ðŸ“Š **Technical Skills Gained**\n",
    "- **Data Processing**: Loading and preprocessing facial image datasets\n",
    "- **Model Architecture**: Designing efficient CNN architectures\n",
    "- **Training Optimization**: Using PyTorch Lightning for streamlined training\n",
    "- **Model Evaluation**: Analyzing performance with confusion matrices and metrics\n",
    "- **Deployment Preparation**: Saving models for real-world applications\n",
    "\n",
    "### ðŸŽ® **Game Integration Ready**\n",
    "Your trained model can now:\n",
    "- **Detect emotions** from facial expressions in real-time\n",
    "- **Integrate seamlessly** with the adventure game\n",
    "- **Enable NPCs** to respond based on your emotional state\n",
    "- **Create immersive** emotion-aware gaming experiences\n",
    "\n",
    "## ðŸ“ˆ **Model Performance Summary**\n",
    "- **Architecture**: Compact CNN with Global Average Pooling\n",
    "- **Parameters**: ~6M (perfect for deployment!)\n",
    "- **Training Features**: Data augmentation, early stopping, learning rate scheduling\n",
    "- **Output**: 6 emotion classes with confidence scores\n",
    "\n",
    "## ðŸš€ **Next Steps & Extensions**\n",
    "\n",
    "### ðŸ”¬ **Advanced Improvements**\n",
    "- **Transfer Learning**: Use pre-trained models like ResNet or EfficientNet\n",
    "- **Attention Mechanisms**: Focus on important facial regions\n",
    "- **Real-time Optimization**: Model quantization for mobile deployment\n",
    "- **Multi-modal Input**: Combine facial expressions with voice tone\n",
    "\n",
    "### ðŸŽ® **Game Integration Ideas**\n",
    "- **Dynamic NPCs**: Characters that adapt to your emotional state\n",
    "- **Emotional Storytelling**: Story branches based on player emotions\n",
    "- **Wellness Features**: Games that encourage positive emotions\n",
    "- **Social Gaming**: Emotion-based multiplayer interactions\n",
    "\n",
    "### ðŸ“š **Learning Resources**\n",
    "- Explore **PyTorch tutorials** for advanced techniques\n",
    "- Study **computer vision research papers** for cutting-edge methods\n",
    "- Practice with **different datasets** (age detection, facial landmarks)\n",
    "- Learn about **model optimization** for production deployment\n",
    "\n",
    "## ðŸŒŸ **The Future is Emotion-Aware**\n",
    "You've just created technology that bridges the gap between human emotions and artificial intelligence. This foundation opens doors to:\n",
    "- **Healthcare applications** (mental health monitoring)\n",
    "- **Educational tools** (adaptive learning systems)\n",
    "- **Entertainment** (emotion-responsive media)\n",
    "- **Accessibility** (assistive technologies)\n",
    "\n",
    "**Keep building, keep learning, and keep pushing the boundaries of what's possible with AI!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
